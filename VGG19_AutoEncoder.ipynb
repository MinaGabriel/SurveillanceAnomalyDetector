{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import os\n",
    "import glob\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Data preprocessing\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert 1-channel grayscale to 3-channel\n",
    "    transforms.Resize((224, 224)),                # Resize to fit VGG19 input dimensions\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "root_dir = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/'\n",
    "\n",
    "def skipping_stride(images_dir, window_size=10, stride=1):\n",
    "    images = sorted(glob.glob(os.path.join(images_dir, '*.tif')))\n",
    "    sequences = []\n",
    "    # if we are predicting LSTM images then should not multiply \n",
    "    for start in range(0, len(images), stride * window_size):\n",
    "        window = images[start: start + stride * window_size:stride]\n",
    "        sequence = [augment_transforms(default_loader(f)) for f in window]\n",
    "        sequence_tensor = torch.stack(sequence, dim=0)\n",
    "        sequences.append(sequence_tensor)\n",
    "    return sequences\n",
    "\n",
    "data = []\n",
    "\n",
    "for subfolder in tqdm(sorted(os.listdir(root_dir)), desc=\"Processing Subfolders\"):\n",
    "    folder_path = os.path.join(root_dir, subfolder)\n",
    "    for stride_value in [1,2]:\n",
    "        sequences = skipping_stride(folder_path, stride=stride_value)\n",
    "        if sequences:\n",
    "            data.extend(sequences)\n",
    "\n",
    "\n",
    "data_tensor = torch.stack(data, dim=0)\n",
    "print(data_tensor.shape)\n",
    "\n",
    "# Create the dataset from the tensor\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tensor(tensor_image):\n",
    "    # CxHxW to HxWxC or getting invalid dimensions\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)  \n",
    "    plt.imshow(tensor_image.numpy())\n",
    "    plt.title('Single Image')\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "# Testing the dataset\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create an iterator from the DataLoader\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "# Fetch the next batch\n",
    "batch = next(train_loader_iter) \n",
    "\n",
    "print(batch[0].shape)\n",
    "\n",
    "plot_image_tensor(batch[0][0][0])  # Plot the first image from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class VGG19Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19Autoencoder, self).__init__()\n",
    "        # Load pre-trained VGG19 model\n",
    "        vgg19_features = models.vgg19(pretrained=True).features\n",
    "\n",
    "        # Using VGG19 as encoder (feature extractor)\n",
    "        self.encoder = nn.Sequential(*list(vgg19_features.children()))\n",
    "\n",
    "        # Decoder (upsampling the image back)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Assuming the output of VGG19 features is 512 channels, you can adjust as necessary\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Use sigmoid for normalizing output to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, c, h, w = x.size()\n",
    "        output_sequence = []\n",
    "        for t in range(seq_len):\n",
    "            # Process each frame in the sequence individually\n",
    "            xt = x[:, t, :, :, :]  # Shape: [batch_size, channels, height, width]\n",
    "            xt = self.encoder(xt)\n",
    "            xt = self.decoder(xt) \n",
    "            output_sequence.append(xt.unsqueeze(1))\n",
    "\n",
    "        output_sequence = torch.cat(output_sequence, dim=1)\n",
    "        return output_sequence\n",
    "\n",
    "# Example usage\n",
    "autoencoder = VGG19Autoencoder()\n",
    "print(autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"train_loader.dataset: {len(train_loader.dataset)}\")\n",
    "print(f\"val_loader.dataset: {len(val_loader.dataset)}\")\n",
    "\n",
    "model = VGG19Autoencoder()  # Assuming ConvLSTM is defined elsewhere\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, eps=1e-6, weight_decay=1e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = 'VGG19_AutoEncoder.pth'\n",
    "\n",
    "# To store loss statistics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for (images,) in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    train_losses.append(average_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Average Training Loss: {average_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (images,) in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, images).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Validation Loss after Epoch {epoch + 1}: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Model saved at epoch {epoch + 1}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = VGG19Autoencoder()\n",
    "model = nn.DataParallel(model)\n",
    "# Load the saved model parameters\n",
    "model.load_state_dict(torch.load('VGG19_ConvLSTM.pth'))\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode if you are using it for inference\n",
    "model.eval()\n",
    "\n",
    "from PIL import Image\n",
    "  \n",
    "files = [f\"Test/Test004/{x:03d}.tif\" for x in range(100,200)]\n",
    "\n",
    "files_iter = iter(files)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load one image for testing\n",
    "image_path = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/' + next(files_iter)\n",
    "image = Image.open(image_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform =   transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale\n",
    "    transforms.Resize((256, 256)),                # Resize if necessary\n",
    "    transforms.ToTensor()                         # Convert to tensor\n",
    "])\n",
    "image = transform(image).to(device)\n",
    "image = image.unsqueeze(1).unsqueeze(1)  # Add a batch dimension at the front \n",
    "image = image.permute(1,2, 0, 3, 4)  # Rearranges to [1, 3, 158, 238, 1]\n",
    "print(image_path)\n",
    "print(image.shape)\n",
    " \n",
    "with torch.no_grad():\n",
    "    output = model(image).to(device)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Convert to NumPy arrays for visualization\n",
    "np_image = image.squeeze().cpu().numpy()\n",
    "np_output = output.squeeze().cpu().numpy()\n",
    "\n",
    "# If the images have channels as the first dimension, transpose them to (H, W, C)\n",
    "if np_image.shape[0] in (1, 3):\n",
    "    np_image = np_image.transpose(1, 2, 0)\n",
    "if np_output.shape[0] in (1, 3):\n",
    "    np_output = np_output.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the absolute difference\n",
    "difference = np.abs(np_image - np_output) \n",
    "\n",
    "# Calculate the MSE\n",
    "mse = np.mean((np_image - np_output) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Create a heatmap to visualize the differences\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np_output, cmap='gray')\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(difference, cmap='hot')\n",
    "plt.title('Difference Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
