{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import os\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import os\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torch.nn.functional as F\n",
    "# Define your transformations\n",
    "augment_transforms = [\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize if necessary\n",
    "        transforms.ToTensor()           # Convert to tensor\n",
    "    ]),\n",
    "]\n",
    "\n",
    "root_direct = os.path.dirname('./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/')\n",
    "sequences = []  # This will hold the training sequences\n",
    "\n",
    "# Define the sequence function\n",
    "def get_sequences_from_folder(folder_path, stride, sequence_length=10):\n",
    "    image_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.tif')])\n",
    "    sequences = []\n",
    "    for start_idx in range(0, len(image_files), stride * sequence_length):\n",
    "        sequence_files = image_files[start_idx:start_idx + stride * sequence_length:stride]\n",
    "        if len(sequence_files) == sequence_length:\n",
    "            sequence = [default_loader(os.path.join(folder_path, f)) for f in sequence_files]\n",
    "            transformed_sequence = [augment_transforms[0](img) for img in sequence]\n",
    "            sequence_tensor = torch.stack(transformed_sequence, dim=0)\n",
    "            sequences.append(sequence_tensor)\n",
    "    return sequences\n",
    "\n",
    "# Generate sequences with different strides\n",
    "for subfolder in os.listdir(root_direct):\n",
    "    subfolder_path = os.path.join(root_direct, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for stride in range(1, 3):  # Here, we use stride values of 1 and 2\n",
    "            sequences.extend(get_sequences_from_folder(subfolder_path, stride))\n",
    "\n",
    "# Stack all the sequences together to create the final dataset tensor\n",
    "sequences_tensor = torch.stack(sequences, dim=0)\n",
    "print(sequences_tensor.shape)\n",
    "\n",
    "# Create the dataset from the tensor\n",
    "dataset = TensorDataset(sequences_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tensor(tensor_image):\n",
    "    # CxHxW to HxWxC or getting invalid dimensions\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)  \n",
    "    plt.imshow(tensor_image.numpy())\n",
    "    plt.title('Single Image')\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "# Testing the dataset\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create an iterator from the DataLoader\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "# Fetch the next batch\n",
    "batch = next(train_loader_iter) \n",
    "\n",
    "print(batch[0].shape)\n",
    "\n",
    "plot_image_tensor(batch[0][0][0])  # Plot the first image from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.gates = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                               out_channels=4 * hidden_channels,  # for input, forget, cell, and output gates\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=self.padding)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        h_cur, c_cur = hidden_state\n",
    "        # print(\"[input_tensor, h_cur]\", [input_tensor.size(), h_cur.size()])\n",
    "        # concatenate along the channel dimension\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        gates = self.gates(combined)\n",
    "\n",
    "        # Split the combined gate tensor into its components\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "        c_next = forget_gate * c_cur + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        # Spatial Encoder\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=128, kernel_size=11, stride=4, padding=(11-1)//2)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=64, kernel_size=5, stride=2, padding=(5-1)//2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "\n",
    "        # Temporal Encoder (ConvLSTM)\n",
    "        self.convlstm1 = ConvLSTMCell(\n",
    "            input_channels=64, hidden_channels=64, kernel_size=3)\n",
    "        self.convlstm2 = ConvLSTMCell(\n",
    "            input_channels=64, hidden_channels=32, kernel_size=3)\n",
    "        self.convlstm3 = ConvLSTMCell(\n",
    "            input_channels=32, hidden_channels=64, kernel_size=3)\n",
    "        # Spatial Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=12, stride=4, padding=4, output_padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=3, kernel_size=11, padding=(11-1)//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and cell states\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        # print(\"x.size()\", x.size())\n",
    "        h1, c1 = self.convlstm1.init_hidden(b, (h//8, w//8))\n",
    "        h2, c2 = self.convlstm2.init_hidden(b, (h//8, w//8))\n",
    "        h3, c3 = self.convlstm3.init_hidden(b, (h//8, w//8))\n",
    "        output_sequence = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            # Spatial Encoder\n",
    "            xt = self.conv1(x[:, t])\n",
    "            xt = self.bn1(xt)\n",
    "            xt = self.dropout1(xt)\n",
    "            xt = F.relu(xt)\n",
    "\n",
    "            xt = self.conv2(xt)\n",
    "            xt = self.bn2(xt)\n",
    "            xt = self.dropout2(xt)\n",
    "            xt = F.relu(xt)\n",
    "\n",
    "            # Temporal Encoder\n",
    "            h1, c1 = self.convlstm1(xt, (h1, c1))\n",
    "            h2, c2 = self.convlstm2(h1, (h2, c2))\n",
    "            h3, c3 = self.convlstm3(h2, (h3, c3))\n",
    "\n",
    "            # Spatial Decoder\n",
    "            xt = self.deconv1(xt)\n",
    "            xt = self.bn3(xt)\n",
    "            xt = self.dropout3(xt)\n",
    "            xt = F.relu(xt)\n",
    "\n",
    "            xt = self.deconv2(xt)\n",
    "            xt = self.bn4(xt)\n",
    "            xt = self.dropout4(xt)\n",
    "            xt = F.relu(xt)\n",
    "\n",
    "            xt = torch.sigmoid(self.conv3(xt))\n",
    "\n",
    "            output_sequence.append(xt.unsqueeze(1))\n",
    "\n",
    "        # Concatenate along the sequence dimension\n",
    "\n",
    "        output_sequence = torch.cat(output_sequence, dim=1)\n",
    "        return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Testing the dataset\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConvLSTM()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = 'best_model.pth'  # Path to save the best model\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for (images,) in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Average Training Loss: {average_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (images,) in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, images).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Loss after Epoch {epoch+1}: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Model saved at epoch {epoch + 1}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Load the best model after training\n",
    "best_model = ConvLSTM()\n",
    "best_model = nn.DataParallel(best_model)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "# Now best_model holds the best model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = torch.load('model.pth')\n",
    "\n",
    "model.to(device)\n",
    "# Dummy input of shape (batch_size, sequence_length, channels, height, width)\n",
    "# Assuming batch_size=1 and sequence_length=10 for this example\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image = batch[0].to(device)\n",
    " \n",
    "\n",
    " \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Convert to NumPy arrays for visualization\n",
    "np_image = image.squeeze().cpu().numpy()\n",
    "np_output = output.squeeze().cpu().numpy()\n",
    "\n",
    "print(np_image.shape)\n",
    "print(np_output.shape)\n",
    "\n",
    "\n",
    "# If the images have channels as the first dimension, transpose them to (H, W, C)\n",
    "if np_image.shape[0] in (1, 3):\n",
    "    np_image = np_image.transpose(1, 2, 0)\n",
    "if np_output.shape[0] in (1, 3):\n",
    "    np_output = np_output.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the absolute difference\n",
    "difference = np.abs(np_image - np_output) \n",
    "\n",
    "# Calculate the MSE\n",
    "mse = np.mean((np_image - np_output) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Create a heatmap to visualize the differences\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np_image[0].transpose(1, 2, 0), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np_output[0].transpose(1, 2, 0), cmap='gray')\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(difference[0].transpose(1, 2, 0), cmap='hot')\n",
    "plt.title('Difference Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dummy input of shape (batch_size, sequence_length, channels, height, width)\n",
    "# Assuming batch_size=1 and sequence_length=10 for this example\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image = batch[0].to(device)\n",
    " \n",
    "\n",
    " \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Convert to NumPy arrays for visualization\n",
    "np_image = image.squeeze().cpu().numpy()\n",
    "np_output = output.squeeze().cpu().numpy()\n",
    "\n",
    "print(np_image.shape)\n",
    "print(np_output.shape)\n",
    "\n",
    "\n",
    "# If the images have channels as the first dimension, transpose them to (H, W, C)\n",
    "if np_image.shape[0] in (1, 3):\n",
    "    np_image = np_image.transpose(1, 2, 0)\n",
    "if np_output.shape[0] in (1, 3):\n",
    "    np_output = np_output.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the absolute difference\n",
    "difference = np.abs(np_image - np_output) \n",
    "\n",
    "# Calculate the MSE\n",
    "mse = np.mean((np_image - np_output) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Create a heatmap to visualize the differences\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np_image[0].transpose(1, 2, 0), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np_output[0].transpose(1, 2, 0), cmap='gray')\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(difference[0].transpose(1, 2, 0), cmap='hot')\n",
    "plt.title('Difference Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 128, 128])\n",
      "torch.Size([10, 128, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define a Conv2d layer\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=512, kernel_size=11, stride=4, padding=(11-1)//2)\n",
    "conv2 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=5, stride=2, padding=(5-1)//2)\n",
    "deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "deconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=6, stride=2, padding=2, output_padding=0)\n",
    "conv3 = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=11, padding=(11-1)//2)\n",
    " \n",
    " \n",
    "input_tensor = torch.randn(10, 64, 64, 64)\n",
    "\n",
    "# Apply the convolutional layer to the input tensor\n",
    "#x = conv1(input_tensor)\n",
    "#print(x.shape)\n",
    "#x = conv2(x)\n",
    "#print(x.shape)\n",
    "# Apply the convolutional layer to the input tensor\n",
    "x = deconv1(input_tensor)\n",
    "print(x.shape)\n",
    "x = deconv2(x)\n",
    "print(x.shape)\n",
    "x = conv3(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define a Conv2d layer\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "            # Start with the last VGG19 feature size and work backwards\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),  # Replace with ConvTranspose2d for finer control\n",
    "            # ... continue mirroring VGG19 layers ...\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=64, kernel_size=3, padding=1)\n",
    "            \n",
    "        )\n",
    " \n",
    " \n",
    "input_tensor = torch.randn(10, 64, 64, 64)\n",
    "\n",
    "\n",
    "x = decoder(input_tensor)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a ConvTranspose2d layer\n",
    "conv_transpose = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "\n",
    "# Dummy tensor for illustration\n",
    "input_tensor = torch.randn(10, 64, 32, 32)\n",
    "\n",
    "# Apply the convolutional transpose layer to the input tensor\n",
    "x = conv_transpose(input_tensor)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(x.shape)  # Should be [10, 64, 64, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ConvLSTM()\n",
    "model = nn.DataParallel(model)\n",
    "# Load the saved model parameters\n",
    "model.load_state_dict(torch.load('ConvLSTM_VGG19_best.pth'))\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode if you are using it for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "  \n",
    "files = [f\"Test/Test006/{x:03d}.tif\" for x in range(120,200)]\n",
    "\n",
    "files_iter = iter(files)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load one image for testing\n",
    "image_path = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/' + next(files_iter)\n",
    "image = Image.open(image_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([ \n",
    "     transforms.Grayscale(num_output_channels=3),\n",
    "     transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "image = transform(image).to(device)\n",
    "image = image.unsqueeze(1).unsqueeze(1)  # Add a batch dimension at the front \n",
    "image = image.permute(1,2, 0, 3, 4)  # Rearranges to [1, 3, 158, 238, 1]\n",
    "\n",
    "print(image.shape)\n",
    " \n",
    "with torch.no_grad():\n",
    "    output = model(image).to(device)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Convert to NumPy arrays for visualization\n",
    "np_image = image.squeeze().cpu().numpy()\n",
    "np_output = output.squeeze().cpu().numpy()\n",
    "\n",
    "# If the images have channels as the first dimension, transpose them to (H, W, C)\n",
    "if np_image.shape[0] in (1, 3):\n",
    "    np_image = np_image.transpose(1, 2, 0)\n",
    "if np_output.shape[0] in (1, 3):\n",
    "    np_output = np_output.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the absolute difference\n",
    "difference = np.abs(np_image - np_output) \n",
    "\n",
    "# Calculate the MSE\n",
    "mse = np.mean((np_image - np_output) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Create a heatmap to visualize the differences\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np_output, cmap='gray')\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(difference, cmap='hot')\n",
    "plt.title('Difference Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1020, 10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import os\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torch.nn.functional as F\n",
    "# Define your transformations\n",
    "augment_transforms = [\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize if necessary\n",
    "        transforms.ToTensor()           # Convert to tensor\n",
    "    ]),\n",
    "]\n",
    "\n",
    "root_direct = os.path.dirname('./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/')\n",
    "sequences = []  # This will hold the training sequences\n",
    "\n",
    "# Define the sequence function\n",
    "def get_sequences_from_folder(folder_path, stride, sequence_length=10):\n",
    "    image_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.tif')])\n",
    "    sequences = []\n",
    "    for start_idx in range(0, len(image_files), stride * sequence_length):\n",
    "        sequence_files = image_files[start_idx:start_idx + stride * sequence_length:stride]\n",
    "        if len(sequence_files) == sequence_length:\n",
    "            sequence = [default_loader(os.path.join(folder_path, f)) for f in sequence_files]\n",
    "            transformed_sequence = [augment_transforms[0](img) for img in sequence]\n",
    "            sequence_tensor = torch.stack(transformed_sequence, dim=0)\n",
    "            sequences.append(sequence_tensor)\n",
    "    return sequences\n",
    "\n",
    "# Generate sequences with different strides\n",
    "for subfolder in os.listdir(root_direct):\n",
    "    subfolder_path = os.path.join(root_direct, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for stride in range(1, 3):  # Here, we use stride values of 1 and 2\n",
    "            sequences.extend(get_sequences_from_folder(subfolder_path, stride))\n",
    "\n",
    "# Stack all the sequences together to create the final dataset tensor\n",
    "sequences_tensor = torch.stack(sequences, dim=0)\n",
    "print(sequences_tensor.shape)\n",
    "\n",
    "# Create the dataset from the tensor\n",
    "dataset = TensorDataset(sequences_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [64, 128, 8, 8], expected input[1, 512, 16, 16] to have 64 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m model \u001b[39m=\u001b[39m ConvLSTM_VGG19()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m output \u001b[39m=\u001b[39m model(image)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv1(h3)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m xt \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(xt)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeconv2(xt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m xt \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(xt)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X42sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv3(xt)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [64, 128, 8, 8], expected input[1, 512, 16, 16] to have 64 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.gates = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                               out_channels=4 * hidden_channels,  # for input, forget, cell, and output gates\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=self.padding)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        h_cur, c_cur = hidden_state\n",
    "        #print(\"[input_tensor, h_cur]\", [input_tensor.size(), h_cur.size()])\n",
    "        # concatenate along the channel dimension\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        #print(\"[combined]\", combined.size())\n",
    "        #print(\"input_channels + hidden_channels\" , self.input_channels + self.hidden_channels)\n",
    "        gates = self.gates(combined)\n",
    "        #print(\"[combined, gates]\", [combined.size(), gates.size()])\n",
    "        # Split the combined gate tensor into its components\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "        #print(\"[input_gate, forget_gate, cell_gate, output_gate]\", [input_gate.size(), forget_gate.size(), cell_gate.size(), output_gate.size()])\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "        c_next = forget_gate * c_cur + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "        #print(\"[h_next, c_next]\", [h_next.size(), c_next.size()])\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM_VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM_VGG19, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG19 model\n",
    "        vgg19 = models.vgg19(pretrained=True).features\n",
    "        # Freeze the first half of the layers\n",
    "        '''\n",
    "        num_layers = len(vgg19)\n",
    "        for layer in vgg19[:num_layers // 2]:  # Freeze the first half\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        '''\n",
    "        self.vgg19_features = vgg19\n",
    "        \n",
    "        self.convlstm1 = ConvLSTMCell(input_channels=512, hidden_channels=64, kernel_size=3) # Adjust the input_channels based on VGG19 output\n",
    "        self.convlstm2 = ConvLSTMCell(input_channels=64, hidden_channels=32, kernel_size=3)\n",
    "        self.convlstm3 = ConvLSTMCell(input_channels=32, hidden_channels=64, kernel_size=3)\n",
    "\n",
    "        # Spatial Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=512, kernel_size=4, stride=2, padding=1, output_padding=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=8, stride=4, padding=2, output_padding=0)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=256, kernel_size=8, stride=4, padding=2, output_padding=0)\n",
    "        \n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=3, kernel_size=11, padding=(11-1)//2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        \n",
    "        h1, c1 = self.convlstm1.init_hidden(b, (8, 8)) # Adjust the size as per the VGG19 output\n",
    "        h2, c2 = self.convlstm2.init_hidden(b, (8, 8))\n",
    "        h3, c3 = self.convlstm3.init_hidden(b, (8, 8))\n",
    "\n",
    "        output_sequence = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            # Pass through the VGG19 spatial encoder\n",
    "            xt = self.vgg19_features(x[:, t]) \n",
    "            #print(xt.shape)\n",
    "            # Temporal Encoder\n",
    "            h1, c1 = self.convlstm1(xt, (h1, c1))\n",
    "            h2, c2 = self.convlstm2(h1, (h2, c2))\n",
    "            h3, c3 = self.convlstm3(h2, (h3, c3))\n",
    "            # Spatial Decoder \n",
    "            xt = self.deconv1(h3)\n",
    "            xt = F.relu(xt)\n",
    "            \n",
    "            xt = self.deconv2(xt)\n",
    "            xt = F.relu(xt)\n",
    "            \n",
    "            xt = self.deconv3(xt)\n",
    "            xt = F.relu(xt)\n",
    "\n",
    "\n",
    "            xt = torch.sigmoid(self.conv3(xt)) \n",
    "            output_sequence.append(xt.unsqueeze(1))\n",
    "\n",
    "        output_sequence = torch.cat(output_sequence, dim=1)\n",
    "        return output_sequence\n",
    "            \n",
    "\n",
    "model = ConvLSTM_VGG19()\n",
    "image = torch.randn(1, 10, 3, 256, 256)\n",
    "\n",
    "output = model(image)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 17:49:36.589356: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-23 17:49:36.653521: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-23 17:49:36.653546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-23 17:49:36.654456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-23 17:49:36.662127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 17:49:37.731575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/ConvLSTM_experiment_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [1/20] Average Training Loss: 0.0147\n",
      "Validation Loss after Epoch 1: 0.0236\n",
      "Model saved at epoch 1\n",
      "Epoch [2/20] Average Training Loss: 0.0064\n",
      "Validation Loss after Epoch 2: 0.0183\n",
      "Model saved at epoch 2\n",
      "Epoch [3/20] Average Training Loss: 0.0055\n",
      "Validation Loss after Epoch 3: 0.0197\n",
      "Epoch [4/20] Average Training Loss: 0.0050\n",
      "Validation Loss after Epoch 4: 0.0211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X43sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X43sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X43sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X43sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minagabriel/Documents/SurveillanceAnomalyDetector/ConvLSTM.ipynb#X43sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m average_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m num_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Testing the dataset\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConvLSTM_VGG19()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5, eps=1e-6)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = 'ConvLSTM_VGG19_best.pth'  # Path to save the best model\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for (images,) in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        #print(\"out:\",outputs.size())   \n",
    "        #print(\"image:\", images.size())   \n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Average Training Loss: {average_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (images,) in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, images).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Loss after Epoch {epoch+1}: {val_loss:.4f}')\n",
    "    \n",
    "    # Log training and validation loss\n",
    "    writer.add_scalar('Loss/train', average_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Model saved at epoch {epoch + 1}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Load the best model after training\n",
    "best_model = ConvLSTM_VGG19()\n",
    "best_model = nn.DataParallel(best_model)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "# Now best_model holds the best model state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
