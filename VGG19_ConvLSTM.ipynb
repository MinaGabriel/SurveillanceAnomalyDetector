{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import os\n",
    "import glob\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Data preprocessing\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert 1-channel grayscale to 3-channel\n",
    "    transforms.Resize((224, 224)),                # Resize to fit VGG19 input dimensions\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "root_dir = './UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/'\n",
    "\n",
    "def skipping_stride(images_dir, window_size=10, stride=1):\n",
    "    images = sorted(glob.glob(os.path.join(images_dir, '*.tif')))\n",
    "    sequences = []\n",
    "    # if we are predicting LSTM images then should not multiply \n",
    "    for start in range(0, len(images), stride * window_size):\n",
    "        window = images[start: start + stride * window_size:stride]\n",
    "        sequence = [augment_transforms(default_loader(f)) for f in window]\n",
    "        sequence_tensor = torch.stack(sequence, dim=0)\n",
    "        sequences.append(sequence_tensor)\n",
    "    return sequences\n",
    "\n",
    "data = []\n",
    "\n",
    "for subfolder in tqdm(sorted(os.listdir(root_dir)), desc=\"Processing Subfolders\"):\n",
    "    folder_path = os.path.join(root_dir, subfolder)\n",
    "    for stride_value in [1,2]:\n",
    "        sequences = skipping_stride(folder_path, stride=stride_value)\n",
    "        if sequences:\n",
    "            data.extend(sequences)\n",
    "\n",
    "\n",
    "data_tensor = torch.stack(data, dim=0)\n",
    "print(data_tensor.shape)\n",
    "\n",
    "# Create the dataset from the tensor\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tensor(tensor_image):\n",
    "    # CxHxW to HxWxC or getting invalid dimensions\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)  \n",
    "    plt.imshow(tensor_image.numpy())\n",
    "    plt.title('Single Image')\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "# Testing the dataset\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create an iterator from the DataLoader\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "# Fetch the next batch\n",
    "batch = next(train_loader_iter) \n",
    "\n",
    "print(batch[0].shape)\n",
    "\n",
    "plot_image_tensor(batch[0][0][0])  # Plot the first image from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional LSTM Cell\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.gates = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                               out_channels=4 * hidden_channels,  # For input, forget, cell, and output gates\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=self.padding)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        h_cur, c_cur = hidden_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        #print(f\"combined shape: {[input_tensor.shape, h_cur.shape]}\")\n",
    "        gates = self.gates(combined)\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "        c_next = forget_gate * c_cur + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channels, height, width, device=self.gates.weight.device))\n",
    "\n",
    "class ConvLSTM_VGG19(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM model with VGG19-based architecture\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM_VGG19, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG19 model\n",
    "        vgg19_features = models.vgg19(pretrained=True).features\n",
    "\n",
    "        # Using VGG19 as encoder (feature extractor)\n",
    "        self.encoder = nn.Sequential(*list(vgg19_features.children()))\n",
    "        \n",
    "        \n",
    "        # ConvLSTM layers\n",
    "        self.convlstm1 = ConvLSTMCell(input_channels=512, hidden_channels=64, kernel_size=3)\n",
    "        self.convlstm2 = ConvLSTMCell(input_channels=64, hidden_channels=32, kernel_size=3)\n",
    "        self.convlstm3 = ConvLSTMCell(input_channels=32, hidden_channels=64, kernel_size=3)\n",
    "\n",
    "        # Spatial Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=10, stride=3, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=9, stride=3, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=9,stride=3, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        h1, c1 = self.convlstm1.init_hidden(b, (7, 7))\n",
    "        h2, c2 = self.convlstm2.init_hidden(b, (7, 7))\n",
    "        h3, c3 = self.convlstm3.init_hidden(b, (7, 7))\n",
    "\n",
    "        output_sequence = []\n",
    "        for t in range(seq_len):\n",
    "            xt = self.encoder(x[:, t])\n",
    "            #print(\"******:\" , xt.shape)\n",
    "            h1, c1 = self.convlstm1(xt, (h1, c1))\n",
    "            h2, c2 = self.convlstm2(h1, (h2, c2))\n",
    "            h3, c3 = self.convlstm3(h2, (h3, c3))\n",
    "            xt = self.decoder(h3)\n",
    "            xt = torch.sigmoid(xt)\n",
    "            output_sequence.append(xt.unsqueeze(1))\n",
    "\n",
    "        output_sequence = torch.cat(output_sequence, dim=1)\n",
    "        return output_sequence\n",
    "\n",
    "# Example usage\n",
    "model = ConvLSTM_VGG19()\n",
    "image = torch.randn(1, 10, 3, 224, 224)\n",
    "output = model(image)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"train_loader.dataset: {len(train_loader.dataset)}\")\n",
    "print(f\"val_loader.dataset: {len(val_loader.dataset)}\")\n",
    "\n",
    "model = ConvLSTM_VGG19()  # Assuming ConvLSTM is defined elsewhere\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, eps=1e-6, weight_decay=1e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = 'VGG19_ConvLSTM.pth'\n",
    "\n",
    "# To store loss statistics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for (images,) in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    train_losses.append(average_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Average Training Loss: {average_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (images,) in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, images).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Validation Loss after Epoch {epoch + 1}: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Model saved at epoch {epoch + 1}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
